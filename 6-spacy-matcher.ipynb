{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [\n",
    "    {'LIKE_EMAIL': True}\n",
    "]\n",
    "matcher.add('EMAIL_ADDRESS',[pattern])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('This is my email address: vprinco@gmail.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(16571425990740197027, 6, 7)]\n"
     ]
    }
   ],
   "source": [
    "#  this is how we find our matches\n",
    "matches = matcher(doc)\n",
    "print(matches)\n",
    "\n",
    "#the output will look like this\n",
    "# [(16571425990740197027, 6, 7)] a set of tupples\n",
    "#  where index 0= 16571425990740197027 is a lexeme\n",
    "#  and 6 is start index and 7 is end index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  now you will see that our label was added to the nlp model with this unique lexime\n",
    "print(nlp.vocab[matches[0][0]].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('data/wiki_mlk.txt','r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "diving deep into matchers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp2 = spacy.load('en_core_web_sm')\n",
    "matcher = Matcher(nlp2.vocab)\n",
    "patterns = [{'POS':'PROPN'}]\n",
    "matcher.add('PROPER_NOUN', [patterns])\n",
    "doc = nlp2(text)\n",
    "matches = matcher(doc)\n",
    "\n",
    "print(len(matches))\n",
    "for match in matches:\n",
    "    print(match, doc[match[1]: match[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  now to grab multi token pronouns we need to make some twicks to the above code\n",
    "#  the OP (Operators and quantifiers)\n",
    "# greedy='LONGEST' which will look for the longest token \n",
    "# you will notice greedy='LONGEST' will arrange the tokens from longest to shortest\n",
    "nlp2 = spacy.load('en_core_web_sm')\n",
    "matcher = Matcher(nlp2.vocab)\n",
    "patterns = [{'POS':'PROPN','OP':'+'}]\n",
    "matcher.add('PROPER_NOUN', [patterns], greedy='LONGEST')\n",
    "doc = nlp2(text)\n",
    "matches = matcher(doc)\n",
    "#  this below line will allow us to sort the tokens to solve the problem of greedy='LONGEST' sorting\n",
    "matches.sort(key= lambda x: x[1])\n",
    "\n",
    "print(len(matches))\n",
    "for match in matches:\n",
    "    print(match, doc[match[1]: match[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we can say we want to get those proper nouns but this time the ones that are followed by a verb.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  this will be the same code but will will edit the pattern \n",
    "nlp2 = spacy.load('en_core_web_sm')\n",
    "matcher = Matcher(nlp2.vocab)\n",
    "patterns = [{'POS':'PROPN','OP':'+'}, {'POS':'VERB'}]\n",
    "matcher.add('PROPER_NOUN', [patterns], greedy='LONGEST')\n",
    "doc = nlp2(text)\n",
    "matches = matcher(doc)\n",
    "matches.sort(key= lambda x: x[1])\n",
    "\n",
    "print(len(matches))\n",
    "for match in matches:\n",
    "    print(match, doc[match[1]: match[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, `and what is the use of a book,' thought Alice `without pictures or conversation?'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open ('data/alice.json','r') as f:\n",
    "    data = json.load(f)\n",
    "text = data[0][2][0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "suppose we are asked to grap all the quotation marks and describe the person speaking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n"
     ]
    }
   ],
   "source": [
    "#  first we will clean our text cz in this document there ` instead of '\n",
    "text = text.replace(\"`\",\"'\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(451313080118390996, 47, 67) 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n"
     ]
    }
   ],
   "source": [
    "#  now we are going to agrab anything in the quotation marks\n",
    "#  we'll use the above codes and make some mod\n",
    "\n",
    "speak_lemmas = ['think', 'say']\n",
    "nlp2 = spacy.load('en_core_web_sm')\n",
    "matcher = Matcher(nlp2.vocab)\n",
    "patterns = [\n",
    "    {'ORTH':\"'\"},\n",
    "    {\"IS_ALPHA\": True, \"OP\":\"+\"},\n",
    "    {\"IS_PUNCT\": True, \"OP\":\"*\"},\n",
    "    {\"ORTH\":\"'\"},\n",
    "    {\"POS\":\"VERB\", \"LEMMA\":{\"IN\":speak_lemmas}},\n",
    "    {\"POS\":\"PROPN\",\"OP\":\"+\"},\n",
    "    {'ORTH':\"'\"},\n",
    "    {\"IS_ALPHA\": True, \"OP\":\"+\"},\n",
    "    {\"IS_PUNCT\": True, \"OP\":\"*\"},\n",
    "    {\"ORTH\":\"'\"},\n",
    "]\n",
    "matcher.add('PROPER_NOUN', [patterns], greedy='LONGEST')\n",
    "doc = nlp2(text)\n",
    "matches = matcher(doc)\n",
    "matches.sort(key= lambda x: x[1])\n",
    "\n",
    "print(len(matches))\n",
    "for match in matches:\n",
    "    print(match, doc[match[1]: match[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(3232560085755078826, 47, 67) 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "(3232560085755078826, 0, 6) 'Well!' thought Alice\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "(3232560085755078826, 57, 68) 'which certainly was not here before,' said Alice\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#  now lets try to add this to owr while book chapters\n",
    "speak_lemmas = [\"think\", \"say\"]\n",
    "text = data[0][2][0].replace( \"`\", \"'\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern1 = [{'ORTH': \"'\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': \"'\"}, {\"POS\": \"VERB\", \"LEMMA\": {\"IN\": speak_lemmas}}, {\"POS\": \"PROPN\", \"OP\": \"+\"}, {'ORTH': \"'\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': \"'\"}]\n",
    "pattern2 = [{'ORTH': \"'\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': \"'\"}, {\"POS\": \"VERB\", \"LEMMA\": {\"IN\": speak_lemmas}}, {\"POS\": \"PROPN\", \"OP\": \"+\"}]\n",
    "pattern3 = [{\"POS\": \"PROPN\", \"OP\": \"+\"},{\"POS\": \"VERB\", \"LEMMA\": {\"IN\": speak_lemmas}}, {'ORTH': \"'\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': \"'\"}]\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern1, pattern2, pattern3], greedy='LONGEST')\n",
    "for text in data[0][2]:\n",
    "    text = text.replace(\"`\", \"'\")\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "    matches.sort(key = lambda x: x[1])\n",
    "    print (len(matches))\n",
    "    for match in matches[:10]:\n",
    "        print (match, doc[match[1]:match[2]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
